{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Ri - Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "import random\n",
    "import nltk as n\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sb\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse trecho de codigo fazemos toda uma pré processamento remove stopwords das noticias que poderam afetar a execução do Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(data):\n",
    "    data[\"noticia\"] = data.titulo + \" \" + data.subTitulo + \" \" + data.conteudo\n",
    "    docs = []\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    for index, doc in data.iterrows():\n",
    "        doc.noticia = doc.noticia.lower()\n",
    "        word_tokens = word_tokenize(doc.noticia)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        news = ' '.join(filtered_sentence)\n",
    "        docs.append(clear_text(news))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "    text = normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    return pattern.sub(' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o conjunto de dados é muito grande temos que utilizar apenas 10 % do seu tamanho,e para isso utilizei a seleçao de documentos do nosso data-set de forma aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = \"https://raw.githubusercontent.com/wendleypf/recuperacao_informacao/master/projeto-ri/data-set/estadao_noticias_eleicao.csv\"\n",
    "data = p.read_csv(file_csv, encoding = \"utf-8\")\n",
    "data = data.replace(np.NAN, \"\")\n",
    "\n",
    "num_lines = len(data)\n",
    "# ~10%\n",
    "size = int(num_lines / 10)\n",
    "skip_idx = random.sample(range(1, num_lines), num_lines - size)\n",
    "data = p.read_csv(file_csv, skiprows=skip_idx, encoding = \"utf-8\")\n",
    "data = data.replace(np.NAN, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = build(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando TF-IDF de cada palavra nos documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qual o numero ideal de clusters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar método Elbow, trata-se de uma técnica interessante para encontrar o valor ideal o número de clusters do nosso agrupamento utilizando o Kmeans.\n",
    "Basicamente o que o método faz é testar a variância dos dados em relação ao número de clusters.\n",
    "É considerado um valor ideal de k quando o aumento no número de clusters não representa um valor significativo de ganho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1,10) # Definindo que o numero de clusters pode variar entre 1 e 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, init='k-means++', max_iter=10000, n_init=100).fit(vector.toarray())\n",
    "    kmeanModel.fit(vector.toarray())\n",
    "    distortions.append(sum(np.min(cdist(vector.toarray(), kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / vector.toarray().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamente podemos ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Método do cotovelo para K ótimo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução do algoritmo Kmeans sobre os documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois da execuçao do método de Elbow podermos ver que o valor K seria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters= 4, init='k-means++', max_iter=10000, n_init=100)\n",
    "km.fit(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo os centroids de cada grupo bem como os labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.labels_\n",
    "centroids = np.array(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud de cada grupo criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dic = {}\n",
    "groups_term_freq = {}\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    groups_dic.setdefault(labels[i], []).append(documents[i])\n",
    "\n",
    "for group in groups_dic.keys():\n",
    "    groups_dic[group] = ' '.join(groups_dic[group])\n",
    "    words = [word.lower() for word in n.word_tokenize(groups_dic[group])]\n",
    "    freq_words = Counter(words)\n",
    "    groups_term_freq[group] = freq_words.most_common()\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white', max_font_size=40).generate(groups_dic[group])\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title('Word Cloud do grupo ' + str(group))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 4 palavras de cada grupo criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups_term_freq.keys():\n",
    "    s = groups_term_freq[group][:4]\n",
    "    y = []\n",
    "    x = []\n",
    "    for i in range(len(s)):\n",
    "        x.append(s[i][0])\n",
    "        y.append(s[i][1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.barh(x, y, color=\"blue\")\n",
    "    plt.xlabel(\"Número de ocorrências\")\n",
    "    plt.title(\"Palavras com a maior frequência no grupo \" + str(group))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
